
TODO what is autoencoder 

TODO why use regularization 

(VAE vs WAE into)
VAE and WAE both minimize two terms: reconstruction and the distance 
between latent codes and the prior distribution (usually normal distr.). 

Reconstruction loss is the same loss used in vanilla autoencoders (i.e. with no regularization ). 
We take the input image and compare it with the image generated by the decoder.

The main difference between VAE and WAE is how we measure the distance between our prior 
the latent distribution.


(VAE Theory)

From practical perspective, VAE can be seen as AE with added regularization. 
Mathematical formulation of VAE is more different. Unlike vanilla AE, which usually formulated as dimensiality reduction method. 

Formal definition of VAE consists of non-deterministic encoder and decoder. 